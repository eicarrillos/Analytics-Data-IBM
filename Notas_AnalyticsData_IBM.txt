Notas del curso de IBM

Modulo 1

PARTE 1. EL ECOSISTEMA DE DATOS MODERNOS
--------------------------------->

Video 2
ecosistema de datos moderno

en la actualidad tenemos 3 tipos de bases de datos 

1. interconectados
2. independientes 
3. en evolución 

de la fuente de datos se pueden obtener 2 tipos de data

1. estructurada 
2. no estructurada

Video 3
Diferencias entre las profesiones relacionadas al tratamiento de datos

Data engienering convert raw data into usable data

Data Analitics use this data to generate insights

Data Scienttists use Data Analytics and Data Engineering to predict the future using
data from the past

Business Analysts and Business Intelligence Analysts use these insights and predictions 
to drive decisions that benefit and grow their business


Video 4
El analisis de datos es el proceso de recopilar, limpiar, analizar y extraer datos---> interpretar resultados y presentar los hallazgos

primera etapa

analisis descriptivo: muestra  los resultados de unos datos pasados

analítica de diagnostico: toma las percepciones del análisis descriptivo para encontrar
                          la causa de la outcome

analisis predictivo: que sucederá?, el historico de datos y las tendencias se utilizan para 
                     predecir los resultados futuros comunmente usando en la predicción de ventas

analisis prescriptivo: responde a la pregunta que se debe hacer al respecto? Analizando desiciones y eventos pasados
                       ejemplo los autos electricos con piloto automotatico, toman desiciones para cambiar de 
                       carril deacuerdo a datos de entorno pasados y presentes


El proceso de un análisis de datos

1. comprensión del problema y del resultado deseado: definir donde estamos y para donde queremos ir 

2. Establecer una métrica clara:  esta etapa del proceso incluye decidir qué se medirá (por ejemplo tomar los datos en ciertas temporadas 
                                  o en intervalos definidos de tiempo)

3. Recopilación de datos: identificados los datos que se requieren, las fuentes de datos de las que se deben extraer estos datos y las mejores 
                          herramientas para el trabajo.

4. Limpieza de datos: una vez recopilados los datos, el siguiente 
                        paso es arreglar los problemas de calidad de los 
                        datos que podrian afectar a la precisión del análisis  este paso limpia los datos tanto de los faltantes
                        como de los valores atípicos,

5. Analisis y minería de datos: Analisis desde diferentes perspectivas para entender las tendencias, correlaciones y encontrar patrones y variaciones
                        interpretando resultados

6. Interpretando Resultados:  despues de analizar los datos y posiblemente investigar más, lo que se puede ser un bucle reiterativo. a medida que 
                        que interpretas los resultados debes evaluar si tu análisis es defendible contra objeciones, y si hay alguna limitación o 
                        circunstancia bajo la cual tu análisis puede no ser cierto.

7. Presentación de las concluciones: estas deben presentarse de manera clara e impacatante para facilitar la toma de deciones del solicitante.

Video 5

En el quinto video se realiza una encuesta a algunos profesionales relacionados con el análisis de datos para saber en su experiencia que consideran que es
el análisis de datos, que en su gran mayoria consiste en una herramienta que nos permite comprobar hipotesis sobre las bases de datos que estamos trabajando
donde este proceso es mecanico abriendo con una extracción, limpieza y tratamiento de datos.

un enfoque interesante es en el que se postula que el analisis de datos esta toda decision que tomamos desde el hecho de levantarno hasta el de pasar la calle 
debido que esa toma de deciones es un acto que realizamos apartir una serie de informacíon que obtenemos de nuestro entorno, es decir el análasis de datos es un
proceso que usamos inconcientemente en el día a día pero ahora pagan para hacerlo a mayor escala de datos.

PARTE 2. EL ROL DE UN ANALISTA DE DATOS
--------------------------------->
Video 1. Responsabilidades de un Analista de datos

Funciones tipícas:
1. Adquirir datos de bases primarias y secundarias  
2. Crear consultas en las bases de datos para obtener los datos nesesarios
3. Filtrar, Limpiar, normalizar y reorganizar los datos en proceso
4. Utilizar herramientas estadisticas para interpetrar los conjuntos de datos
5. Utilizar las herramientas estadisticas para encontrar patrones y correlaciones de datos
6. Analizar patrones en conjuntos de datos complejos e interpretar tendencias.
7. Preparar informes y graficos  que comuniquen eficazmente las tendencias y los patrones
8. Crear la documentación adecuada para definir y demostrar los pasos del procesos de análisis

Habilidades de un Analista de datos
1. Habilidades Tecnicas
1.1. Experiencia en el uso de hojas de excel
1.2. Dominio y experiencia en sofware de analisis estadistico como IBM cognos,  SSPS, power Bi, Tableu.
1.3. Dominio de alguno de los lenguajes de programación como R, Python, y en algunos casos c++, java o MATLAB
1.4. Buen conocimiento en SQL y capacidad para trabajar en bases de datos relacionales y NoSQL
1.5. La capacidad de acceder y Extraer datos del repositorio de datos, como Data marts, Data Warehouses, Data lake and Data pipelines 
1.6. Familiaridad con la herramientas de procesamientos de Big Data como Hadoop, Hive, Spark

2.Habilidades Funcionales
2.1. Dominio de la estadistica para ayudarte a analizar los datos, validar tu análisis, e identificar las faltas y errores tecnicos
2.2. Habilidades analiticas que te ayuden a investigar e interpretar los datos, teorizar y hacer pronosticos.
2.3. Habilidades de resolución de problemas, por que en ultima instancia, el objetivo final del análisis de datos es resolver problemas
2.4. Habilidades de indagación, que son escenciales para el processo de descubrimiento, es decir, para comprender un problema desde la perspectivas
     de varios interesados y usuarios, por que el proceso de análisis de datos comienza realmente con una clara articulación del enunciado del problema
     y el resultado deseado.
2.5. Habilidades de visualización de datos que ayuden a decidir las tecnicas e instrumentos que presentan los hallazgos de manera eficaz en función de la audiencia.
     el tipo de datos, el contexto y el objetivo final del análisis.
2.6. Habilidades de gestion de proyectos, para manejar el proceso, personal, dependencias y plazos de la iniciativa.

3. Habilidades Blandas.
3.1. Trabajar en colaboración con equipos empresariales y multifuncionales.
3.2. Comunicar de manera efectiva para informar y presentar tus hallazgos,
3.3. Contar una historia atractiva y convincente, conseguir apoyo y aceptación de tu trabajo.
3.4. CURIOSIDAD
3.5. INTUICIÓN
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULO 2

El trabajo de un analista de datos tiene una infraestructura definida por los siguientes items:
     1. Recopilación de datos:
          la base de datos se puede clasificar en las siguientes categorias
          1.1 Estructurados
               Estos bases son aquellas que tienen un formato rigido y ordenado de tal manera que pueden organizarse en filas y columnas es tal el caso de las hojas de calculo (excel), 
               y las bases de DATOS
          1.2 Semi-Estructurados
               Estos datos son una mezcla de datos organizados con un formato rigido y otro no estructurado, como ejemplo de estos datos tenemos los correos dado que si tomamos el remitente y 
               el destinatario como una base de datos claramente esta es estructurada pero el contenido del correo no es un dato organizado.
          1.3 No estructurados
               Este tipo de datos es en el cual se nos es imposible reducir la información a filas o columnas o mejor dicho no se puede organizar, como ejemplo tenemos las fotos, los videos, los pdf'saber
               ect, ahora como bien es sabido una foto no es mas que la imagen formada por un conjunto de pixeles los cuales pueden ser nombrados pero no organizados
          Es importante saber con que base de datos se cuentan para poder determinar las herramientas a usar en el processo de recolección.
          las bases de datos pueden ser obtenidas de diferentes fuentes, que van desde:
               -bases de datos relacionales
               -Bases de datos no relacionales
               -APIs 
               -Servicios web
               -Data streams
               -Plataforma de Redes 
               -Dispositivos con sensores.
          Tras conocer la fuente de datos es importante conocer el tipo de repositorio adecuado a construir para optimizar la distibución de datos, estos se eligen deacuerdo a estas 3 caracteristicas:
               -Tipo de datos
               -Formato 
               -Fuentes de datos
          y segun estas tres caracteristicas podemos formar un repositorio de datos los cuales se clasifican en:
               -Databases 
               -Data Warehouses
               -Data marts
               -Data Lakes
               -Big Data Stores
          El ecosistema de datos incluye lenguajes que se clasifican como:
               -Lenguaje de consultas        (SQL)
                    SQL(Structured Query Language)= es un lenguaje de consulta estructurado diseñado para acceder y manipular de principalmente
                    , aunque no exclusivamente, bases de datos relacionales, usando SQL podemos escribir un conjunto de instrucciones para realizar operaciones como
                    1-Insertar      -|
                     -Actualizar    -| Registros en una base de datos
                     -Eliminar      -|
                    2-Crear Nuevas bases de datos
                     -Tablas
                     -vistas
                    3-Escribir procedimientos almacenados
                         lo que significa que se puede escribir un conjunto de instrucciones y llamarlas para usarlas posteriormente
                    -Ventajas de usar SQL
                         1. Es portable  y se puede usar independientemente de la Plataforma
                         2. Pueden usarse para consultar datos en una amplia variedad de bases de datos y repositorios
                         3. Tiene una sintaxis simple similar a la del idioma ingles
                         4. Su sintaxis permite que los desarroladores escriban programas con menos líneas que con algunos otros lenguajes de programación
                              usando claves básicas como select, insert, into y update.
                         5. Puede extraer grandes cantidades de datos de forma rápida y eficiente 
               -Lenguajes de programación    (Python)
                    Python: Lenguaje Multi paradigma que soporta multiples lenguajes, es la ostia de los programas de alto nivel ya que tiene una curva de aprendizaje muy alta 
                         como punto importante a resaltar es el uso de las miles de librerias que maneja, que por ejemplo sirven para:
                              1.Pandas: para Data Wranling o la limpieza y preparación de datos.
                              2.Numpy and Scipy:: para el análisis estadistico .
                              3.BeautifulSoup and Scrapy: para el web Scraping.
                              4.Matplotlib and Seaborn: para la visualización de datos.
                              5.Opency: para el procesamiento de imagenes.
               -Lenguajes de shell y programación de scripts     (comandos de la shell)
     En resumen
     -> los datos estructurados son los qu están bien organizados en formatos que pueden almacenarse en bases de datos  y se presentan a métodos y herramientas de análisis de datos estandar
     -> Los datos Semi-estructurados son los que están de alguna manera organizados y dependen de metaetiquetas para agruparlos y jerarquizarlos
     -> Los datos no estructurados son los que están organizados convencionalmente en forma de filas y columnas

     2. Limpieza de data
     3. Analizar
     4. Visualizar data


Las herramientas utilizadas en el Data analysts van desde:

--recopilar         --Data Wranling               --Data mining       --Data visualization
--extraer           --(Preparación de datos)      
--transformar       --limpieza
--cargar datos      

Entendendimiento de los diferentes tipos de formatos

Algunos Formatos a ver son:
-Formatos de archivo de texto delimitado o CSV
     Estos son los formatos a los cuales cada linea tiene un valor separado por delimitadores, donde un delimitador es un secuencia de uno o más caracteres para especificar el limite
     CSV = Comma-Separated-values
     TSV = Tab-Separated-Values
-Hojas de calculo, Excel Open XML o XLSL
     XML = Lenguaje de Marcado Extensible
-Lenguaje de Marcado Extensible o .XML
-Formato de Documento portatil o pdf
-Notación de objeto JavaScript o JSON

FUENTES DE DATOS

Dentro de las más comunes podemos encontrar:
     1. Bases de datos relacionales
          algunas de ellas son: 
          1.1. SQL
          1.2. ORACLE
          1.3. MySQL
          1.4. IBM DB2
          estas bases de datos tienen como objetivo guardar la información de forma estructurada, los datos almacenados en bases de datos y data Warehouses
          se pueden utilizar  como fuente para el análisis 
     2. Flat File or XML Datasets (archivos .txt y .xlsl)
          2.1. Flat File: Archivos .txt, este tipo de archivos cada fila tiene un valor o un dato separado por un caracter como por ejemplo la coma 
               el cual es muy conocido como csv(Comma-Separated-Value), otros son .tsv(Tab-Separated-Value) que el delimitador es la tecla Tabulador,
               existen multiples delimitadores como el punto, algún caracter, número entre otros.
               una diferencia muy marcada entre este formato y las bases de datos relacionales es el hecho de que los datos almacenados en archivos planos
               y archivos XML están guardados en un misma tabla a diferencia de las bases de datos relacionales.
          2.2. Spreadsheet files: es un tipo especial de Flat file, debido a su estructura tabular donde puedes organizar los datos en fila y columnas, estos formatos 
               contienen multiples hojas de trabajo, que pueden asignarse a diferentes tablas, otra caracteristica relevante de este formato es que en ellas 
               se guardan tanto datos como formulas.
               El mas usado es XLSL de microsoft
          2.3. XML(Xtensible-markup-lenguage): contienen valores de datos que se identifican o marcan mediante etiquetas, en constraste de los flat files que son 
               asignados a una tabla, los archivos XML permiten estructuras de datos más complejas donde es posible la jerarquización del mismo, algunos de los usamos
               comunes son: los datos de las encuestas en linea, extractos bancarios y otros conjuntos de datos no estructurados
     3. APIs and Web Services: (Interfaces de Programas de aplicación), muchos proveedores de datos ofrecen APIs y servicios web, estos servicios reciben las consultas de 
          de los usuarios y la devuelven en diferentes formatos como: JSON, XML, HTML entre otros.
          APIs populares para obtener datos son:
          3.1. twiter and facebook
          3.2. del mercado de valores
          3.3. de busqueda y validación de datos
     4. Web Scraping (extracción de datos web): Se utiliza para extraer datos relevantes de fuentes no estructuradas,
          tambien conocido como screen scraping y web harvestin, este permite descargar datos especificos de páginas web
          en base a parámetros definidos, los extractores web pueden entre otras cosas, extraer texto, información de contacto
          , imagenes, videos.
          algunos usos comunes son:
          4.1. Recolectar detalles de productos de minoristas, fabricantes y sitios web de comercio electronico para proporcionar 
               comparaciones de precios.
          4.2. Generar oportunidades de venta a travéz de fuentes de datos publicas
          4.3. Extraer datos de publicaciones y autores en diversos foros y comunidades 
          4.4. Recolectar conjuntos de datos de entrenamiento y prueba para modelos de machine learning
     las herramientas mas populares para el Web Scraping son:
     -BeautifulSoup
     -Scrapy
     -Pandas
     -Selenium
     5. Data Streams and feeds: los flujos de datos son ampliamente utilizados para agregar flujos constantes de datos que fluyen 
          de fuentes como instrumentos, diapositivas y aplicaciones loT, gps de automoviles, estos datos suelen tener una marca de tiempo
          y tambien están etiquetados geograficamente con su localización, una de las maneras en las que se pueden aprovechar los Data Streams 
          es:
          -indicadores bursátiles y de mercado para el comercio financiero.
          -flujos de comercio minorista para predecir la demanda y la gestion de la cadena de suministro
          -flujos de video y vigilancia para la detección de amenazas
          -Canales de redes sociales para el análisis de sentimientos
          -flujos de datos de sensores para la supervición de maquinaria industrial o agrícola,
          -flujos de clics en la web para supervisar el rendimiento de la web y mejorar el diseño
          -Eventos de vuelos en tiempo real para hacer nuevas reservaciones y reprogramaciónes
          Las teconologias más importantes para el procesamiento de Data streams son:
          1-Kafka
          2-Apache Spark Streaming
          3-Apache Storm
     6. RSS: los canales RSS(or Really simple Syndication) feeds 
          son otra fuente de datos popular, normalmente se utilizan para captar datos actualizados de foros en linea y sitios de noticias
          los cuales se actualizan de forma continua, mediante un lector de feeds, que es una interfaz que convierte los archivos de text RSS
          en un flujo de datos actualizados, las actualizaciones se transmiten a los dispositivos del usuario

Parte 2: Comprensión de los Repositorios de Datos y plataformas de grandes volumenes de datos


          






