Notas del curso de IBM

Modulo 1

PARTE 1. EL ECOSISTEMA DE DATOS MODERNOS
--------------------------------->

Video 2
ecosistema de datos moderno

en la actualidad tenemos 3 tipos de bases de datos 

1. interconectados
2. independientes 
3. en evolución 

de la fuente de datos se pueden obtener 2 tipos de data

1. estructurada 
2. no estructurada

Video 3
Diferencias entre las profesiones relacionadas al tratamiento de datos

Data engienering convert raw data into usable data

Data Analitics use this data to generate insights

Data Scienttists use Data Analytics and Data Engineering to predict the future using
data from the past

Business Analysts and Business Intelligence Analysts use these insights and predictions 
to drive decisions that benefit and grow their business


Video 4
El analisis de datos es el proceso de recopilar, limpiar, analizar y extraer datos---> interpretar resultados y presentar los hallazgos

primera etapa

analisis descriptivo: muestra  los resultados de unos datos pasados

analítica de diagnostico: toma las percepciones del análisis descriptivo para encontrar
                          la causa de la outcome

analisis predictivo: que sucederá?, el historico de datos y las tendencias se utilizan para 
                     predecir los resultados futuros comunmente usando en la predicción de ventas

analisis prescriptivo: responde a la pregunta que se debe hacer al respecto? Analizando desiciones y eventos pasados
                       ejemplo los autos electricos con piloto automotatico, toman desiciones para cambiar de 
                       carril deacuerdo a datos de entorno pasados y presentes


El proceso de un análisis de datos

1. comprensión del problema y del resultado deseado: definir donde estamos y para donde queremos ir 

2. Establecer una métrica clara:  esta etapa del proceso incluye decidir qué se medirá (por ejemplo tomar los datos en ciertas temporadas 
                                  o en intervalos definidos de tiempo)

3. Recopilación de datos: identificados los datos que se requieren, las fuentes de datos de las que se deben extraer estos datos y las mejores 
                          herramientas para el trabajo.

4. Limpieza de datos: una vez recopilados los datos, el siguiente 
                        paso es arreglar los problemas de calidad de los 
                        datos que podrian afectar a la precisión del análisis  este paso limpia los datos tanto de los faltantes
                        como de los valores atípicos,

5. Analisis y minería de datos: Analisis desde diferentes perspectivas para entender las tendencias, correlaciones y encontrar patrones y variaciones
                        interpretando resultados

6. Interpretando Resultados:  despues de analizar los datos y posiblemente investigar más, lo que se puede ser un bucle reiterativo. a medida que 
                        que interpretas los resultados debes evaluar si tu análisis es defendible contra objeciones, y si hay alguna limitación o 
                        circunstancia bajo la cual tu análisis puede no ser cierto.

7. Presentación de las concluciones: estas deben presentarse de manera clara e impacatante para facilitar la toma de deciones del solicitante.

Video 5

En el quinto video se realiza una encuesta a algunos profesionales relacionados con el análisis de datos para saber en su experiencia que consideran que es
el análisis de datos, que en su gran mayoria consiste en una herramienta que nos permite comprobar hipotesis sobre las bases de datos que estamos trabajando
donde este proceso es mecanico abriendo con una extracción, limpieza y tratamiento de datos.

un enfoque interesante es en el que se postula que el analisis de datos esta toda decision que tomamos desde el hecho de levantarno hasta el de pasar la calle 
debido que esa toma de deciones es un acto que realizamos apartir una serie de informacíon que obtenemos de nuestro entorno, es decir el análasis de datos es un
proceso que usamos inconcientemente en el día a día pero ahora pagan para hacerlo a mayor escala de datos.

PARTE 2. EL ROL DE UN ANALISTA DE DATOS
--------------------------------->
Video 1. Responsabilidades de un Analista de datos

Funciones tipícas:
1. Adquirir datos de bases primarias y secundarias  
2. Crear consultas en las bases de datos para obtener los datos nesesarios
3. Filtrar, Limpiar, normalizar y reorganizar los datos en proceso
4. Utilizar herramientas estadisticas para interpetrar los conjuntos de datos
5. Utilizar las herramientas estadisticas para encontrar patrones y correlaciones de datos
6. Analizar patrones en conjuntos de datos complejos e interpretar tendencias.
7. Preparar informes y graficos  que comuniquen eficazmente las tendencias y los patrones
8. Crear la documentación adecuada para definir y demostrar los pasos del procesos de análisis

Habilidades de un Analista de datos
1. Habilidades Tecnicas
1.1. Experiencia en el uso de hojas de excel
1.2. Dominio y experiencia en sofware de analisis estadistico como IBM cognos,  SSPS, power Bi, Tableu.
1.3. Dominio de alguno de los lenguajes de programación como R, Python, y en algunos casos c++, java o MATLAB
1.4. Buen conocimiento en SQL y capacidad para trabajar en bases de datos relacionales y NoSQL
1.5. La capacidad de acceder y Extraer datos del repositorio de datos, como Data marts, Data Warehouses, Data lake and Data pipelines 
1.6. Familiaridad con la herramientas de procesamientos de Big Data como Hadoop, Hive, Spark

2.Habilidades Funcionales
2.1. Dominio de la estadistica para ayudarte a analizar los datos, validar tu análisis, e identificar las faltas y errores tecnicos
2.2. Habilidades analiticas que te ayuden a investigar e interpretar los datos, teorizar y hacer pronosticos.
2.3. Habilidades de resolución de problemas, por que en ultima instancia, el objetivo final del análisis de datos es resolver problemas
2.4. Habilidades de indagación, que son escenciales para el processo de descubrimiento, es decir, para comprender un problema desde la perspectivas
     de varios interesados y usuarios, por que el proceso de análisis de datos comienza realmente con una clara articulación del enunciado del problema
     y el resultado deseado.
2.5. Habilidades de visualización de datos que ayuden a decidir las tecnicas e instrumentos que presentan los hallazgos de manera eficaz en función de la audiencia.
     el tipo de datos, el contexto y el objetivo final del análisis.
2.6. Habilidades de gestion de proyectos, para manejar el proceso, personal, dependencias y plazos de la iniciativa.

3. Habilidades Blandas.
3.1. Trabajar en colaboración con equipos empresariales y multifuncionales.
3.2. Comunicar de manera efectiva para informar y presentar tus hallazgos,
3.3. Contar una historia atractiva y convincente, conseguir apoyo y aceptación de tu trabajo.
3.4. CURIOSIDAD
3.5. INTUICIÓN
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULO 2

Parte 1. EL ECOSISTEMA DE DATOS Y LENGUAJES PARA PROFESIONALES DE DATOS.
---------------------------------->

El trabajo de un analista de datos tiene una infraestructura definida por los siguientes items:
     1. Recopilación de datos:
          la base de datos se puede clasificar en las siguientes categorias
          1.1 Estructurados
               Estos bases son aquellas que tienen un formato rigido y ordenado de tal manera que pueden organizarse en filas y columnas es tal el caso de las hojas de calculo (excel), 
               y las bases de DATOS
          1.2 Semi-Estructurados
               Estos datos son una mezcla de datos organizados con un formato rigido y otro no estructurado, como ejemplo de estos datos tenemos los correos dado que si tomamos el remitente y 
               el destinatario como una base de datos claramente esta es estructurada pero el contenido del correo no es un dato organizado.
          1.3 No estructurados
               Este tipo de datos es en el cual se nos es imposible reducir la información a filas o columnas o mejor dicho no se puede organizar, como ejemplo tenemos las fotos, los videos, los pdf'saber
               ect, ahora como bien es sabido una foto no es mas que la imagen formada por un conjunto de pixeles los cuales pueden ser nombrados pero no organizados
          Es importante saber con que base de datos se cuentan para poder determinar las herramientas a usar en el processo de recolección.
          las bases de datos pueden ser obtenidas de diferentes fuentes, que van desde:
               -bases de datos relacionales
               -Bases de datos no relacionales
               -APIs 
               -Servicios web
               -Data streams
               -Plataforma de Redes 
               -Dispositivos con sensores.
          Tras conocer la fuente de datos es importante conocer el tipo de repositorio adecuado a construir para optimizar la distibución de datos, estos se eligen deacuerdo a estas 3 caracteristicas:
               -Tipo de datos
               -Formato 
               -Fuentes de datos
          y segun estas tres caracteristicas podemos formar un repositorio de datos los cuales se clasifican en:
               -Databases 
               -Data Warehouses
               -Data marts
               -Data Lakes
               -Big Data Stores
          El ecosistema de datos incluye lenguajes que se clasifican como:
               -Lenguaje de consultas        (SQL)
                    SQL(Structured Query Language)= es un lenguaje de consulta estructurado diseñado para acceder y manipular de principalmente
                    , aunque no exclusivamente, bases de datos relacionales, usando SQL podemos escribir un conjunto de instrucciones para realizar operaciones como
                    1-Insertar      -|
                     -Actualizar    -| Registros en una base de datos
                     -Eliminar      -|
                    2-Crear Nuevas bases de datos
                     -Tablas
                     -vistas
                    3-Escribir procedimientos almacenados
                         lo que significa que se puede escribir un conjunto de instrucciones y llamarlas para usarlas posteriormente
                    -Ventajas de usar SQL
                         1. Es portable  y se puede usar independientemente de la Plataforma
                         2. Pueden usarse para consultar datos en una amplia variedad de bases de datos y repositorios
                         3. Tiene una sintaxis simple similar a la del idioma ingles
                         4. Su sintaxis permite que los desarroladores escriban programas con menos líneas que con algunos otros lenguajes de programación
                              usando claves básicas como select, insert, into y update.
                         5. Puede extraer grandes cantidades de datos de forma rápida y eficiente 
               -Lenguajes de programación    (Python)
                    Python: Lenguaje Multi paradigma que soporta multiples lenguajes, es la ostia de los programas de alto nivel ya que tiene una curva de aprendizaje muy alta 
                         como punto importante a resaltar es el uso de las miles de librerias que maneja, que por ejemplo sirven para:
                              1.Pandas: para Data Wranling o la limpieza y preparación de datos.
                              2.Numpy and Scipy:: para el análisis estadistico .
                              3.BeautifulSoup and Scrapy: para el web Scraping.
                              4.Matplotlib and Seaborn: para la visualización de datos.
                              5.Opency: para el procesamiento de imagenes.
               -Lenguajes de shell y programación de scripts     (comandos de la shell)
     En resumen
     -> los datos estructurados son los qu están bien organizados en formatos que pueden almacenarse en bases de datos  y se presentan a métodos y herramientas de análisis de datos estandar
     -> Los datos Semi-estructurados son los que están de alguna manera organizados y dependen de metaetiquetas para agruparlos y jerarquizarlos
     -> Los datos no estructurados son los que están organizados convencionalmente en forma de filas y columnas

     2. Limpieza de data
     3. Analizar
     4. Visualizar data


Las herramientas utilizadas en el Data analysts van desde:

--recopilar         --Data Wranling               --Data mining       --Data visualization
--extraer           --(Preparación de datos)      
--transformar       --limpieza
--cargar datos      

Entendendimiento de los diferentes tipos de formatos

Algunos Formatos a ver son:
-Formatos de archivo de texto delimitado o CSV
     Estos son los formatos a los cuales cada linea tiene un valor separado por delimitadores, donde un delimitador es un secuencia de uno o más caracteres para especificar el limite
     CSV = Comma-Separated-values
     TSV = Tab-Separated-Values
-Hojas de calculo, Excel Open XML o XLSL
     XML = Lenguaje de Marcado Extensible
-Lenguaje de Marcado Extensible o .XML
-Formato de Documento portatil o pdf
-Notación de objeto JavaScript o JSON

FUENTES DE DATOS

Dentro de las más comunes podemos encontrar:
     1. Bases de datos relacionales
          algunas de ellas son: 
          1.1. SQL
          1.2. ORACLE
          1.3. MySQL
          1.4. IBM DB2
          estas bases de datos tienen como objetivo guardar la información de forma estructurada, los datos almacenados en bases de datos y data Warehouses
          se pueden utilizar  como fuente para el análisis 
     2. Flat File or XML Datasets (archivos .txt y .xlsl)
          2.1. Flat File: Archivos .txt, este tipo de archivos cada fila tiene un valor o un dato separado por un caracter como por ejemplo la coma 
               el cual es muy conocido como csv(Comma-Separated-Value), otros son .tsv(Tab-Separated-Value) que el delimitador es la tecla Tabulador,
               existen multiples delimitadores como el punto, algún caracter, número entre otros.
               una diferencia muy marcada entre este formato y las bases de datos relacionales es el hecho de que los datos almacenados en archivos planos
               y archivos XML están guardados en un misma tabla a diferencia de las bases de datos relacionales.
          2.2. Spreadsheet files: es un tipo especial de Flat file, debido a su estructura tabular donde puedes organizar los datos en fila y columnas, estos formatos 
               contienen multiples hojas de trabajo, que pueden asignarse a diferentes tablas, otra caracteristica relevante de este formato es que en ellas 
               se guardan tanto datos como formulas.
               El mas usado es XLSL de microsoft
          2.3. XML(Xtensible-markup-lenguage): contienen valores de datos que se identifican o marcan mediante etiquetas, en constraste de los flat files que son 
               asignados a una tabla, los archivos XML permiten estructuras de datos más complejas donde es posible la jerarquización del mismo, algunos de los usamos
               comunes son: los datos de las encuestas en linea, extractos bancarios y otros conjuntos de datos no estructurados
     3. APIs and Web Services: (Interfaces de Programas de aplicación), muchos proveedores de datos ofrecen APIs y servicios web, estos servicios reciben las consultas de 
          de los usuarios y la devuelven en diferentes formatos como: JSON, XML, HTML entre otros.
          APIs populares para obtener datos son:
          3.1. twiter and facebook
          3.2. del mercado de valores
          3.3. de busqueda y validación de datos
     4. Web Scraping (extracción de datos web): Se utiliza para extraer datos relevantes de fuentes no estructuradas,
          tambien conocido como screen scraping y web harvestin, este permite descargar datos especificos de páginas web
          en base a parámetros definidos, los extractores web pueden entre otras cosas, extraer texto, información de contacto
          , imagenes, videos.
          algunos usos comunes son:
          4.1. Recolectar detalles de productos de minoristas, fabricantes y sitios web de comercio electronico para proporcionar 
               comparaciones de precios.
          4.2. Generar oportunidades de venta a travéz de fuentes de datos publicas
          4.3. Extraer datos de publicaciones y autores en diversos foros y comunidades 
          4.4. Recolectar conjuntos de datos de entrenamiento y prueba para modelos de machine learning
     las herramientas mas populares para el Web Scraping son:
     -BeautifulSoup
     -Scrapy
     -Pandas
     -Selenium
     5. Data Streams and feeds: los flujos de datos son ampliamente utilizados para agregar flujos constantes de datos que fluyen 
          de fuentes como instrumentos, diapositivas y aplicaciones loT, gps de automoviles, estos datos suelen tener una marca de tiempo
          y tambien están etiquetados geograficamente con su localización, una de las maneras en las que se pueden aprovechar los Data Streams 
          es:
          -indicadores bursátiles y de mercado para el comercio financiero.
          -flujos de comercio minorista para predecir la demanda y la gestion de la cadena de suministro
          -flujos de video y vigilancia para la detección de amenazas
          -Canales de redes sociales para el análisis de sentimientos
          -flujos de datos de sensores para la supervición de maquinaria industrial o agrícola,
          -flujos de clics en la web para supervisar el rendimiento de la web y mejorar el diseño
          -Eventos de vuelos en tiempo real para hacer nuevas reservaciones y reprogramaciónes
          Las teconologias más importantes para el procesamiento de Data streams son:
          1-Kafka
          2-Apache Spark Streaming
          3-Apache Storm
     6. RSS: los canales RSS(or Really simple Syndication) feeds 
          son otra fuente de datos popular, normalmente se utilizan para captar datos actualizados de foros en linea y sitios de noticias
          los cuales se actualizan de forma continua, mediante un lector de feeds, que es una interfaz que convierte los archivos de text RSS
          en un flujo de datos actualizados, las actualizaciones se transmiten a los dispositivos del usuario

Parte 2: cOMPRENSIÓN DE LOS REPOSITORIOS DE DATOS Y PLATAFORMAS DE GRANDES VOLÚMENES DE DATOS
------------------------------------>

overview de los repositorios más usados tales como:
     Databases: Son aquellos repositorios en los cuales se almancenan datos de forma estructurada para una futura introducción, consulta, modificación eliminación  y 
               de la data almacenada allí. y un Sistema de Administración de Bases de Datos, o (DBMS), es un conjunto de programas que crea y mantiene las bases de datos
               Permite almacenar, modificar y extraer información de la base de datos mediante una función llamada consulta.
               aunque bases de datos y DBMS signifiquen cosas diferentes, se suelen utilizar insitintivamente, pero aunque existen muchas bases de datos 
               en la elección de ella influyen muchos factores como : 
               -Tipo de datos
               -Estructura de los datos
               -Mecanismos de consulta
               -Requisitos de latencia
               -velocidades de transacción 
               -Uso previsto de datos
               
               Las databases pueden clasificarse en dos grandes grupos los cuales son 
               1. Relational Databases: estas se basan principalmente en principios organizativos de los archivos planos, con:
                         - datos organizados en forma tabular de fila y columnas
                         - Una estructura bien definida.
                         - optimizadas para operaciones de consulta de datos que implican muchas tablas y volúmnes de datos mucho mayores,(siendo esta una de la mayores diferencias entre los archivos planos y las DBMS)
                         - SQL es el DBMS mas frecuente para el almacenamiento de bases de datos relacionales.
                    RDBMS=(Relational Database Management Systems)
                         Las filas son los registros y las columnas los atributos
                         ejemplos de RDBMS:
                         -Open-Source con soporte interno 
                         -Open-Source con soporte comercial
                         -Commercial closed-Source
                    ventajas de las RDBMS
                         + Crear información significativa al unir tablas.
                         + Flexibilidad para hacer cambios mientras la databases estan en uso
                         + Reducción de las redundancias: para permitir relaciones bien definidas entre tablas
                         + Facilidad para las copias de seguridad, ofrecen opciones de exportación e impotación sencillas.
                         + Cumplimiento ACID significa Atomicidad, Consistencia, Aislamiento y Durabilidad, el cumplimiento de ACID siginifica que los datos siguen siendo 
                         precisos apesar de los fallos y las transacciones de la base de datos se procesan de forma fiable.
                    Casos de uso de RDBMS
                         * Procesamiento de Transacciones Online (OLTP): Las aplicaciones OLTP se centran en tareas orientadas a las transacciones que se ejecutan a gran ritmo 
                              y el por que las databases se ajustan tan bien a las OLTP es por:
                              -Pueden dar cabida a un gran número de usuarios.
                              -Admiten la posibilidad de insertar, actualizar o eliminar pequeñas cantidades de datos
                              -Admiten consultas y actualizaciones frecuentes, asi como respuestas rápidas.
                         * Data Warehouses: en un entorno de base de datos relacionales pueden optimizarse para el procesamiento analítico(OLAP) 
                         * IoT Solutions: Las soluciones del internet de las cosas, require velocidad así como capacidad de recopilar y procesar datos de los dispositivos edge,
                              que nesecitan solución de base de datos ligera.
                    Limitaciones de las RDBMS
                         + No funcionan bien con datos semiestructurados y no estructurados
                         + Para la migración entre dos RDBMS, los esquemas y el tipo de datos tienen que ser idénticos entre las tablas de origen y el destinatario
                         + las bases de datos relacionales tiene un límite en la longitud de los campos de datos, lo que significa que si se intenta introducir en un campo más de información
                              de la que puede contener, la información no se almacenará.
               2. Non-Relational Databases: Tambien conocidas como NoSQL son caracterizadas por:
                         -Surgen en respuesta al volumen, diversidad y velocidad con que se generan hoy en día, principalmente influenciadas por los avances en la computación en la nube.
                         -Construidas para ofrecer velocidades, flexibilidad y escala, 
                         -las bases de datos no relacionales permitieron almacenar datos sin esquemas o de forma librerias
                         -NoSQL se utiliza ampliamente para procesar Big Data.
                         
                         NoSQL(no only SQL): es una base de datos no relacinal creada con las siguientes caracteristicas:
                                        + para construir modelos de datos específicos 
                                        + para tener esquemas flexibles que permtien a los programadores crear y 
                                          manejar aplicaciones modernas.
                                        + para no utilizar un diseño tradicional de base de datos de columna y fila 
                                        + Para no utilizar el lenguaje de consulta estructurado (o SQL) para consultar los datos, aunque algunos pueden admitir interfaces de SQL.
                              el NoSQL permite almacenar datos sin esquema o de forma libre, según el modelo que se utiliza para almacenar los datos, hay cuatro tipos comunes de bases de 
                              datos NoSQL:
                                   -Almacén de clave-valor (key-value store).
                                        Los datos de una base de datos key-value store se almacenan como una colección de pares de clave-valor
                                        + La clave representa un atributo de los datos y es un identificador único
                                        + Tanto las claves como los valores pueden ser cualquier cosa, desde simples enteros o cadenas hasta complejos documentos JSON.
                                        + Son excelentes para guardar los datos de la sesión y las preferencias de los usuarios, hacer recomendaciones en tiempo real y publicidad
                                                  dirigida, y el almacenamiento en memoria de datos
                                             sin embargo si se quieren consultar los datos sobre un valor de datos específico, se nesecitan:
                                             - Consultar los datos sobre un valor de datos especifico.
                                             - Relaciones entre los valores de datos.
                                             - Multiples claves únicas.
                                             para estos casos las key-value store pueden no ser adecuadas
                                        algunos de los ejemplos más conocidos para esta categoria son:
                                             -redis 
                                             -Memcached
                                             -DynamoDB
                                   -Basada en documentos (Document Based).
                                        -Estas almacenan cada registro y sus datos asociados en un solo documento
                                        -Permiten una indización flexible, potentes consultas ad hoc y análisis de las colecciones de documentos.
                                        -Son más adecuadas para las plataformas de comercio electrónico, almacenamiento de registros médicos, plataformas de CRM y plataformas de análisis
                                             sin embargo si lo que se desea consultar es:
                                             - Consultas de busqueda complejas
                                             - Transacciones de múltiples operaciones.
                                             para estos casos puede que los repositorios Documents based no sea la mejor opción.
                                        algunos de los ejemplos más conocidos para esta categoria son:
                                             -MongoDB
                                             -DocumentsDB
                                             -CouchDB
                                             -Cloudant
                                   -Basada en columnas (column based).
                                        - Almacenan los datos en celdas agrupadas como columnas de datos en lugar de filas
                                        - La agrupación lógica de las columnas, es decir, las xolunas a las que se suelen acceder juntas, se denominan familia de columnas.
                                        - Todas las celdas correspondientes a una columna son guardadas como una entrada continua del disco, haciendo que el acceso sea más fácil y rápido.
                                        - Pueden ser excelentes para los sistemas que se requieren intensas solicitudes de escritura, almacenamiento de datos de series temporales, datos meteorológicos
                                             y datos IoT
                                             Sin embargo si tienes que:
                                             -usar consultas complejas
                                             -cambiar tus patrones de consulta con frecuencia
                                             para estos casos puede que los repositorios colunm based no sean los más adecuados.
                                        Algunos ejemplos de tecnologias que usan este tipo de repositorios es:
                                             -Cassandra
                                             -Apache HBASE

                                   -Basada en gráficos (Graph based).
                                        + Las bases de datos basadas en gráficos utilizan un modelo gráfico para representar y almacenar datos
                                        + Son particularmente útiles para visualizar, analizar y encontrar conexiones entre diferentes fragmentos de datos
                                        + Un ejemplo es un diagrama circular el cual cada nodo contiene datos y cada flecha indicando la relación entre ellos
                                        + Son una excelente opción para almacenar datos con multiples relaciones, ejemplo:
                                             - Social networks  
                                             - Recomendaciones de productos en tiempo real.
                                             - Networks diagrams
                                             - Detección de fraudes
                                             - Access management
                                             sin embargo si lo que quires es:
                                                  * Procesar grandes volumenes de transacciones
                                             puede que este repositorio no sea adecuado.
                                        Algunas teconologias que manejan estos repositorios son:
                                             - Neo4j
                                             - Cosmos DB
                         
                         Ventajas de los repositorios NoSQL:
                              - La principal ventaja es su capacidad para manejar grandes volumenes de datos estructurados, semiestructurados y no estructuradosa
                              - La capacidad de ejecutarse como sistemas distribuidos escalados en múltiples centros de datos. lo que permite aprovechar la infraestructura de computación en la nube
               
               

     Data Warehouse: un Data Warehouse funciona como un repositorio central que fusionala información procedente de fuentes dispares y la consolida mediante el proceso de Extracción, Transformación y carga (ETL) 
               a un nivel muy alto, el proceso ETL ayuda a 
                    -Extraer datos de diferentes fuentes de datos,
                    -Transformar los datos a un estado limpio y utilizable
                    -Cargar los datos en el repositorio de datos de la empresa.
               Relacionados con los Data Warehouses están los conceptos de Data Marts y Data Lakes, que historicamente han sido repositorios relacionales
               Optaremos por usar un Data Warehouse cuando se tienen cantidades masivas de datos operacionales que nesecitan estar disponibles para la presentación de informes
               Las caracteristicas de un Data warehouse son:
                    + Es un instrumento multipropósito de análisis operacional y de rendimiento.
                    + Sirven como la única fuente de verdad, almacenamiento de datos actuales e históricos.
          Data Marts: Es una subsección  del Data Warehouse, creado especificamente para una función de negocios concreta, propósito o comunidad de usuarios, la idea es proporcionar 
               a lo interesados los datos más pertinentes para ellos, cuando lo necesiten. por ejemplo, los equipos de ventas o financieros que acceden a los datos para elaborar sus informes y proyecciones trimestrales,
               los Data Marts ofrecen:
                    + Capacidades análiticas para un área restringida del Data Warehouse
                    + Seguridad aislada y rendimiento aislado
                    + El papel más importante de un Data Mart es la generación de informes y análisis especificos de la empresa.
          Data Lakes: Un Data Lake es un repositorio de almacenamiento que puede albergar grandes cantidades de datos estructurados, semiestructurados y no estructurado en su formato nativo,
                    clasificados y etiquetados con metadatos, Así, mientras que un Data Warehouse almacena datos procesados para una necesidad específica, un Data Lake es un conjunto de datos 
                    en bruto donde:
                         + cada elemento de datos recibe un identificador único y es etiquetado con metaetiquetas para su uso posterior.
                         + La Data de un Data lake es seleccionada y organizada basada en el uso del caso que se necesite.
                         + A diferencia de los Data Warehouses, un Data Lake retendría todos los datos de la fuente, sin ninguna exclusión y los datos podrían incluir todo tipo de fuentes y tipos de datos.
                         + A vesces tambien se utilizan como área para la preparación de un data Warehouse.
                         + El papel más importante de un Data Lake es el análisis predictivo y avanzado.
     ETL=(Extract, Transform, and load) Es la forma en que los datos en bruto se convierten en datos listos para el análisis.
          - Es un procesos automatizado ene le que se recogen datos en bruto de fuentes identificadas,
          - Se extrae información que se ajusta a las necesidades de información y análisis.
          - Se limpia, estandariza y transforma esos datos en un formato que se puede utilixar en el contexto de la organización.
          - Carga de la data en un repositorio de datos.
               Extract: La extracción es el paso en el que se recopilan los datos desde las ubicaciones de las funtes para su transformación.
                         puede ser atravez de:
                         - Procesamiento por lotes, lo que significa que los datos de origen se mueven en grandes fragmentos desde la fuente hasta el sistema de destino a intervalos programados.
                         Herramientas para el procesamiento:
                              + Stitch
                              + Blendo
                         - Procesamiento de flujo (Stream processing): que significa que los datos de la fuente se extraen en tiempo real de la misma y se transforman mientras están en transito y antes.
                              de que se carguen en el repositorio de datos.
                          Herramientas para el procesamiento:
                              + Apache Samza
                              + Apache Storm
                              + Apache Kafka
               Transform: La transformación implica la ejecución de reglas y funciones que convierten los datos en bruto en los datos que pueden usarse para el análisis
                         Por ejemplo:
                              - Haciendo que los formatos de fecha y las unidades de medida sean consistentes en todos los datos de origen,
                              - Eliminando los datos duplicados
                              - Filtrando los datos que no se necesitan
                              - Enriqueciendo los datos: por ejemplo dividiendo el nombre completo en nombre, segundo nombre y apellidos
                              - Estableciendo relaciones clave entre las tablas
                              - Aplicando reglas de negocio y validación de datos.
               Load: La carga es el paso en el que los datos procesados se transportan aun sistema de destino o a un repositorio de datos
                         Podria ser:
                              + Carga inicial (Initial loading), es decir, rellenar todos los datos del repositorio.
                              + Carga incremental (Incremental loading), es decir, aplicar periódicamente actualizaciones y modificaciones continuas según sea necesario.
                              + Refresco completo (Full refresh), borrar el contenido de una o más tablas y volver a cargarlas con datos
                         La verificación de la carga checks for:
                              - La comprobación de datos en busca de datos faltantes o nulos.
                              - El rendimiento del servidos
                              - Supervición de fallos

     Big Data Stores: Es una estructura de almacenamiento distribuida de computación para almacenar, escalar y procesar grandes volumenes de datos.
                    se refiere a los volumenes dinámicos, Grandes y dispares de datos creados por personas, herramientas y maquinas. Require una tecnología nueva, innovadora y escalable para 
                    recopilar, alojar y procesar análiticamente la enorme cantidad de datos recopilados con el fin de obtener percepciones del negocio en tiempo real que se relacionen con consumidores
                    riesgo, beneficios, rendimientos, gestion de la productividad y un mayor valor para el accionista.
                    Las V del  big data:
                         -Velocidad: es la rapidez con la que se acumulan los datos. los datos se generan extremadamente rápido en un proceso que nunca se detiene.
                         -Volumen: es la escala de los datos o el aumento de cantidad de datos almacenados
                         -Variedad: es la diversidad de datos.
                         -Veracidad: es la calidad y el origen de los datos y su conformidad con los hechos y la exactitud, los atributos incluyen:
                                        - Consistencia
                                        - Completitud
                                        - Integridad
                                        - Ambigüedad
                         -Valor: es nuestra capacidad y necesidad de convertir los datos en valor

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
MODULO 3

PARTE 1. RECOPILACIÓN Y TRANSFORMACIÓN DE DATOS
----------------------------->
Identificando datos para el Análisis.

Proceso para identificar los datos:
     Paso 1. Determinar la información que quieres recopilar
          en este paso se toman desiciones como:
          1.a-) La información específica que se necesitan
          1.b-) Los posibles origenes de esta Data
     Paso 2. Definir un plan para la recopilación de datos.
          2.a-) Algunos de los datos que se necesitan pueden ser necesarios de manera continua y otros a lo largo de un período de tiempo definido.
          2.b-) En este paso también puedes definir cuántos datos serían suficientes para que pudieras llegar a un análisis creible.
          2.c-) Tambien se puede utilizar este paso para definir las dependencias, riesgos, plan de mitigación y varios otros
     Paso 3. Determinar los métodos de recopilación de datos, en este paso determinaras los métodos de recopilación de datos que necesitas.
          3.a-) Definiras como recabar los datos de las fuentes de datos que has identificado.
               -Los metodos dependeran de:
                    + La fuente de DATOS
                    + El tipo de datos
                    + El plazo en que se necesitan los datos
                    + El volumén de los mismos.
La identificación de la data, la fuente de esos datos y las practicas que tu empleas para obtener esos datos tienen implicaciones para:
     - Calidad: Trabajar con datos de fuentes dispares son tener en cuenta lo bien que cuadran respecto a la metrica de calidad puede conducir al fracaso
               para ser fiables, los datos deben estar:
                    - libres de errores 
                    - precisos
                    - completos
                    - relevantes
                    - Accesibles
               Tambien hay que estar atento a las cuestiones relativas de la Governance:
                    - Seguridad
                    - Regulación 
                    - Cumplimiento  
     - Seguridad: Los datos que se recopilen deben cumplir los requisitos de:
               - Confidencialidad
               - Licencias de uso
               - Cumplimiento de las normas establecidas.
          es necesario planificar:
               - Verificaciones
               - Validaciones
               - Seguimiento auditable
               
     - Privacidad:

Fuentes de datos:

las fuentes de datos pueden ser internas o externas a la organización pueden ser:
     -primarias: esta se refiere a la información obtenida directamente por uno mismo apartir de la fuente,
               -pueden ser de fuentes internas como datos de la organización, CRM, HR, aplicaciones de flujo de trabajo
               -Podrian incluir datos que obtienes directamente por medio de encuestas, entrevistas, charlas, observaciones y grupos de debate.
     -secundarias: los datos secundarios se refieren a la información obtenida de fuentes existentes.
               -Bases de datos externas
               -Artículos de investigación, publicaciones, material de formación y búsquedas en investigación, publicaciones, material de formación y busquedas de internet o registros financieros disponibles como datos publicos.
               -Datos reunidos externamente mediante encuestas, entrevistas, coloquios, observaciones y grupos de debate.   
     -terceros: Los datos de terceros son los que se adquiren de agregadores que reunen datos de diversas fuentes y los combinan en conjuntos de datos completos con el único fin de venderlos.

     Diferentes fuentes de donde se podrian estar recolectando datos
          -Aplicaciones internas para gestionar sus procesos, flujos de trabajo y clientes.
          -Las bases de datos externas están disponibles por suscripción o para su compra. Un número significativo de empresas han pasado o se están pasando a la nube, que se está convirtiendo cada vez más en una fuente de acceso a la información
           en tiempo real y a percepciones bajo demanda.
          -La web es una fuente de datos de acceso público que está a disposición de las empresas y de los individuos para uso gratuito o comercial.
               pueden ser:
               + Libros de texto
               + Registros gubernamentales
               + Documentos y artículos
          - Redes sociales y plataformas interactivas como Facebook, Twitter, Google, YouTube e Instagram. se utilizan cada vez más para obtener datos y opiniones de los usuarios. Las empresas están utilizando estas fuentes de datos para obtener conocimientos cuantitativos y cualitativos. Clientes actuales
            y potenciales.
          - Datos de sensores generados por dispositivos vestibles, edificios inteligentes, ciudades inteligentes, telefonos inteligentes, dispositivos médicos, incluso los electrodomésticos son una fuente de datos muy utilizada.
          - Intercambio de datos: es una fuente de datos de terceros que implica el intercambio voluntario de datos entre los proveedores de datos y los consumidores:
               + Datos procedentes de aplicaciones para negocios
               + Dispositivos de sensores
               + Actividad de redes sociales.
               + datos de localización 
               + Datos de comportamiento de los consumidores.
          -Las encuestas: ellas recogen información mediante cuestionarios distribuidos a un grupo selecto de personas. por ejemplo, midiendo el interés de los clientes existentes en gastar en una versión actualizada de un producto. Las encuestas pueden realizarse en la web o en papel.
          -Los datos del censo tambien son una fuente comúnmente utilizada para recopilar datos de los hogares, como por ejemplo, datos de riqueza e ingresos, o datos de población.
          -Las entrevistas son una fuente para reunir datos cualitativos, como las opiniones y experiencias de los participantes.
          -Estudios de observación: los estudios incluyen la vigilania de los participantes en un entorno específico o mientras realizan una tarea concreta.

Cómo recopilar e Importar Datos:

RECOPILACIÓN DE DATOS

Para recopilar e importar datos como bases de datos, la web, datos de sensores, intercambios de datos, y otras diversas fuentes
Tambien aprenderemos sobre la importación de datos a diferentes tipos de repositorios de datos.

     SQL: Es un lenguaje de consulta usado para extraer información de bases de datos relacionales
          SQL ofrece comandos sencillos para:
          - especificar lo que se debe recuperar de la base de datos, 
          - La tabla de la que se debe extraer.
          - agrupar los registros con valores coincidentes.
          - dictar la secuencia en la que se muestran los resultados de la consulta.
          - limitar el número de resultados
     NoSQL: algunas bases de datos no relacionales vienen con sus propias herramientas de consulta como CQL para:
          -Cassandra
          -GraphQL
          -Neo4J 
     APIs: Tambien se utilizan comúnmente para extraer datos de diversas fuentes 
          -Se invocan desde las aplicaciones que requieren los datos y acceden a un punto final que contien los datos, los puntos finales pueden incluir bases de datos, servicios web y mercados de datos
          - tambien usan la validación de datos.
          Por ejemplo:
               + Un analista puede utilizar una API para validar direcciones postales y códigos postales.
     web scraping: Tambien conocido como screen scraping o web harvesting, se utiliza para descargar datos especificos de una web en función de parámetros definidos.

          - el web scraping se utiliza para extraer datos como texto, información de contacto, imagenes, videos podcast  y articulos de productos a partir de un sitio web.
          - Las fuentes RSS son otro recurso que se suele utilizar para recabar datos actualizados de foros online y sitios de noticias en los qu se actualizan de forma continua.
     Data streams son una fuente popular para agregar flujos constantes de datos que provienen de fuentes como:
          -instrumentos.
          -dispositivos y aplicaciones IoT.
          -Datos de Gps de automoviles.
     
     Data exchanges: las plataformas de Intercambio de datos permiten el intercambio de datos entre los proveedores de datos y los consumidores de datos.
          - Los intercambios de datos tienen un conjunto de normas, protocolos y formatos de intercambio bien definidos y pertinentes para el intercambio de datos.
          - Facilitan el intercambio de datos.
          -garantizan el mantenimiento de la seguridad y la gobernanza
          - Proporcionan flujos de trabajo para la concesión de licencias de datos, anonimización y protección de la información personal, marcos jurídicos y un entorno de análisis protegido.
          entre los ejemplos de plataforma de intercambio de datos populares se encuentran:
               +AWS Data Exchange
               +Crunchbase
               +Lotame
               +Snowflake
     other sources: para las tendencias de marketing y el gasto en publicidad, por ejemplo, se sabe que empresas de investigación como Forrester y Business Insider proporcionan datos fiables.
          las empresas de investigación y asesoramiento como Gartner y Forrester son fuentes de amplia confianza para la orientación estratégica y operacional.

IMPORTAR DATOS:

Los repositorios de datos específicos están optimizados para determinados tipos de datos
     -Las bases de datos relacionales almacenan datos estructurados con un esquema bien definido.
               si utilizas una  base de datos relacional como sistema destino, solo podrás almacenar datos estructurados, como datos de sistemas OLTP, Hojas de cálculo, formularios online,
               sensores, registros de red y web.
               -los datos estructurados tambien se pueden almacenar en NoSQL
               -las bases de datos NoSQL y los Data Lakes constituyen una buen opción para almacenar y manipular grandes volúmenes de datos no estructurados, los data lakes pueden albergar todos los tipos de datos y esquemas.
               -Las herramientas ETL y los data lakes proporcionan funciones automatizadas que facilitan el proceso de importación de datos.
               -Herramientas como Talend e informatíca y lenguajes de programación como Python y R y sus bibliotecas se utilizan ampliamente para importar datos 